// Copyright Epic Games, Inc. All Rights Reserved.

/*=============================================================================
	BuildPerPageDrawCommands.usf.usf: 
=============================================================================*/

#include "../Common.ush"
#include "PageOverlap.ush"
#include "ProjectionCommon.ush"
#include "../Nanite/NaniteDataDecode.ush"
#include "../InstanceCulling/InstanceCullingCommon.ush"
#include "../InstanceCulling/InstanceCullingLoadBalancer.ush"
#include "../WaveOpUtil.ush"


#ifndef NEAR_CLIP
#define NEAR_CLIP 1
#endif

// Stored in an unordered fashion, needs to be reorganized later
struct FVisibleInstanceCmd
{
	uint PackedPageInfo;
	uint InstanceId;
	uint IndirectArgIndex;
};

uint PackPageInfo(FPageInfo PageInfo)
{
	// TODO: Why not do address translation here? Physical pages need fewer bits.
	uint4 PackedData = 0U;
	uint BitPos = 0U;
	WriteBits(PackedData, BitPos, PageInfo.VirtualPage.x, VSM_LOG2_LEVEL0_DIM_PAGES_XY);
	WriteBits(PackedData, BitPos, PageInfo.VirtualPage.y, VSM_LOG2_LEVEL0_DIM_PAGES_XY);
	WriteBits(PackedData, BitPos, PageInfo.ViewId, VSM_PACKED_PAGE_INFO_VIEW_ID_BITS);

	return PackedData.x;
}


RWStructuredBuffer<FVisibleInstanceCmd> VisibleInstancesOut;
uint FirstPrimaryView;
uint NumPrimaryViews;
RWStructuredBuffer<uint> VisibleInstanceCountBufferOut;
uint InstanceSceneDataSOAStride;
uint DynamicInstanceIdOffset;
uint DynamicInstanceIdMax;


StructuredBuffer<FDrawCommandDesc> DrawCommandDescs;

RWBuffer<uint> DrawIndirectArgsBufferOut;

RWStructuredBuffer<uint> OutInvalidatingInstances;
uint NumInvalidatingInstanceSlots;

void MarkInstanceForInvalidation(uint InstanceId)
{
	// Note: InstanceId can be higher than NumInvalidatingInstanceSlots because of Dynamic primitives and their instances as they are added later in the frame
	//       Skipping them is ok, because they invalidate when they are removed (being transient).
	if (InstanceId < uint(NumInvalidatingInstanceSlots))
	{
		uint InstanceIdWordOffset = InstanceId / 32U;
		uint InstanceIdWordMask = 1U << (InstanceId % 32);

		uint PreviousWordMask = 0U;
		InterlockedOr(OutInvalidatingInstances[1 + NumInvalidatingInstanceSlots + InstanceIdWordOffset], InstanceIdWordMask, PreviousWordMask);
		// If the bit was not set, then we need to add it to the list of things to invaliate
		if ((PreviousWordMask & InstanceIdWordMask) == 0U)
		{
			uint OutOffset = 0U;
			WaveInterlockedAddScalar_(OutInvalidatingInstances[0], 1U, OutOffset);
			OutInvalidatingInstances[1U + OutOffset] = InstanceId;
		}
	}
}

void WriteCmd(uint2 VirtualPage, uint MipViewId, uint InstanceId, uint IndirectArgIndex)
{
	FPageInfo PageInfo;
	PageInfo.VirtualPage = VirtualPage;
	PageInfo.ViewId = MipViewId;

	FVisibleInstanceCmd VisibleInstanceCmd;
	VisibleInstanceCmd.PackedPageInfo = PackPageInfo(PageInfo);
	VisibleInstanceCmd.InstanceId = InstanceId;
	VisibleInstanceCmd.IndirectArgIndex = IndirectArgIndex;

	uint VisibleInstanceOutputOffset = 0U;
	WaveInterlockedAddScalar_(VisibleInstanceCountBufferOut[0], 1U, VisibleInstanceOutputOffset);
	VisibleInstancesOut[VisibleInstanceOutputOffset] = VisibleInstanceCmd;
}

[numthreads(NUM_THREADS_PER_GROUP, 1, 1)]
void CullPerPageDrawCommandsCs(uint3 GroupId : SV_GroupID, int GroupThreadIndex : SV_GroupIndex)
{
	// TODO: Support batching? Currently VSM are culled before each draw. Batching could work explicitly or using the same deferred mechanism as other InstanceCullingContexts.
	//       However, these would need to be kept separate, and also means all instance data must be pre-allocated for all the lights (and cannot be re-used between lights).
	//       For supporting many small lights, we will certainly need batching, but perhaps they will use a different path anyway.
	FInstanceWorkSetup WorkSetup = InstanceCullingLoadBalancer_Setup(GroupId, GroupThreadIndex, 0);
	if (!WorkSetup.bValid)
	{
		return;
	}

	const bool bDynamicInstanceDataOffset = (WorkSetup.Item.Payload & 1U) != 0U;
	uint InstanceDataOffset = WorkSetup.Item.InstanceDataOffset;

	if (bDynamicInstanceDataOffset)
	{
		InstanceDataOffset += DynamicInstanceIdOffset;
		checkSlow(InstanceDataOffset + uint(WorkSetup.LocalItemIndex) < DynamicInstanceIdMax);
	}


	uint InstanceId = InstanceDataOffset + uint(WorkSetup.LocalItemIndex);
	uint IndirectArgIndex = WorkSetup.Item.Payload >> 1U;
	FDrawCommandDesc DrawCommandDesc = DrawCommandDescs[IndirectArgIndex];

	const bool bNearClip = (NEAR_CLIP != 0);

	// Load relevant instance data
	FInstanceSceneData InstanceData = GetInstanceSceneData(InstanceId, InstanceSceneDataSOAStride);
	const bool bHasMoved = GetGPUSceneFrameNumber() == InstanceData.LastUpdateSceneFrameNumber || DrawCommandDesc.bMaterialMayModifyPosition;
	const uint PageFlagMask = bHasMoved ? VSM_ALLOCATED_FLAG : VSM_UNCACHED_FLAG;

	uint ThreadTotalForAllViews = 0;
	// Loop over views and output visible instance (i.e., those that overlap a valid page)
	for (uint PrimaryViewId = FirstPrimaryView; PrimaryViewId < FirstPrimaryView + NumPrimaryViews; ++PrimaryViewId)
	{
		FNaniteView NaniteView = GetNaniteView(PrimaryViewId);
		const uint2 TargetViewSize = uint2(NaniteView.ViewSizeAndInvSize.xy);

		float4x4 LocalToTranslatedWorld = LWCMultiplyTranslation(InstanceData.LocalToWorld, NaniteView.PreViewTranslation);
		float4x4 LocalToClip = mul(LocalToTranslatedWorld, NaniteView.TranslatedWorldToClip);

		FFrustumCullData Cull = BoxCullFrustum(InstanceData.LocalBoundsCenter, InstanceData.LocalBoundsExtent, LocalToClip, bNearClip, false);

		if (Cull.bIsVisible)
		{
			// Loop over mip levels and count number of output visible instances, also retain a bit for each mip level (used to skip empty levels in output loop)
			for (uint MipLevel = 0; MipLevel < uint(NaniteView.TargetNumMipLevels); ++MipLevel)
			{
				uint MipViewId = MipLevel * NumPrimaryViews + PrimaryViewId;
				FNaniteView MipView = GetNaniteView(MipViewId);
				uint VirtualShadowMapId = uint(MipView.TargetLayerIndex);

				FScreenRect Rect = GetScreenRect(MipView.ViewRect, Cull, 4);

				uint FlagMask = VSM_NON_NANITE_FLAG | (bHasMoved ? VSM_ALLOCATED_FLAG : VSM_UNCACHED_FLAG);

				if (OverlapsAnyValidPage(VirtualShadowMapId, MipLevel, MipView.ViewRect.xy, Rect, FlagMask))
				{
					uint4 RectPages = uint4( MipView.ViewRect.xyxy + Rect.Pixels ) >> VSM_LOG2_PAGE_SIZE;

					// Clip to actually allocated pages
					// TODO: move this to be done as part of or before the overlap test?
					uint4 AllocatedBounds = PageRectBounds[VirtualShadowMapId * VSM_MAX_MIP_LEVELS + MipLevel];
					RectPages.xy = max(RectPages.xy, AllocatedBounds.xy);
					RectPages.zw = min(RectPages.zw, AllocatedBounds.zw);
					if (all(RectPages.zw >= RectPages.xy))
					{
						++ThreadTotalForAllViews;
						WriteCmd(RectPages.xy, MipViewId, InstanceId, IndirectArgIndex);
					}
				}
			}
		}
	}

	// If the instance has WPO or PDO and rendered to any mip, queue for invalidation. This will be processed the before next frame (after a new HZB is built)
	// and invalidate any visible pages.
	if (ThreadTotalForAllViews > 0U && DrawCommandDesc.bMaterialMayModifyPosition)
	{
		MarkInstanceForInvalidation(InstanceId);
	}

	// Accumulate total number of instances for each indirect argument, is also used to allocate space and output compact range of instances later
	InterlockedAdd(DrawIndirectArgsBufferOut[IndirectArgIndex * INDIRECT_ARGS_NUM_WORDS + 1], ThreadTotalForAllViews);
}

Buffer<uint> DrawIndirectArgsBuffer;
RWBuffer<uint> InstanceIdOffsetBufferOut;
RWStructuredBuffer<uint> OutputOffsetBufferOut;
RWStructuredBuffer<uint> TmpInstanceIdOffsetBufferOut;
uint NumIndirectArgs;

/**
 * Separate pass to allocate space, needs to run once the final space requirements are known. We buffer the page/instance-draw info and reshuffle later.
 * TODO: Possibly just re-run the culling process in the output pass, saves storing stuff, but may cost more and runs the risk of the passes disagreeing e.g., due to rounding or whatever.
 */
[numthreads(NUM_THREADS_PER_GROUP, 1, 1)]
void AllocateCommandInstanceOutputSpaceCs(uint IndirectArgIndex : SV_DispatchThreadID)
{
	if (IndirectArgIndex < NumIndirectArgs)
	{
		uint CommandInstanceCount = DrawIndirectArgsBuffer[IndirectArgIndex * INDIRECT_ARGS_NUM_WORDS + 1];
		uint CommandInstanceOffset = 0U;
		if (CommandInstanceCount > 0U)
		{
			InterlockedAdd(OutputOffsetBufferOut[0], CommandInstanceCount, CommandInstanceOffset);
		}
		InstanceIdOffsetBufferOut[IndirectArgIndex] = CommandInstanceOffset;
		// Store second copy for use during output pass (as we need the first offset buffer during the actual rendering)
		TmpInstanceIdOffsetBufferOut[IndirectArgIndex] = CommandInstanceOffset;
	}

	// Also set up indirect dispatch args for the output pass (OutputCommandInstanceLists)
	//if (IndirectArgIndex == 0)
	//{
	//	uint NumVisibleInstances = VisibleInstanceCountBuffer[0];
	//	// ...dispatch args to process all the visible instances
	//}
}

StructuredBuffer<FVisibleInstanceCmd> VisibleInstances;
StructuredBuffer <uint> VisibleInstanceCountBuffer;
//RWStructuredBuffer<uint> TmpInstanceIdOffsetBufferOut;
RWStructuredBuffer<uint> InstanceIdsBufferOut;
RWStructuredBuffer<uint> PageInfoBufferOut;


[numthreads(NUM_THREADS_PER_GROUP, 1, 1)]
void OutputCommandInstanceListsCs(uint VisibleInstanceIndex : SV_DispatchThreadID)
{
	uint NumVisibleInstances = VisibleInstanceCountBuffer[0];

	if (VisibleInstanceIndex < NumVisibleInstances)
	{
		FVisibleInstanceCmd VisibleInstanceCmd = VisibleInstances[VisibleInstanceIndex];

		// Scatter the instance ID & other data.
		uint InstanceIdOutputOffset = 0;
		InterlockedAdd(TmpInstanceIdOffsetBufferOut[VisibleInstanceCmd.IndirectArgIndex], 1U, InstanceIdOutputOffset);
		// TODO: maybe repack as uint2 since that might be better for these type of presumably scalar loads.
		InstanceIdsBufferOut[InstanceIdOutputOffset] = VisibleInstanceCmd.InstanceId;
		PageInfoBufferOut[InstanceIdOutputOffset] = VisibleInstanceCmd.PackedPageInfo;
	}
}

